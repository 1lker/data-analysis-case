{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb2f931-7d40-4a25-bad3-742f600b60ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in ./case-env/lib/python3.10/site-packages (0.14.3)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in ./case-env/lib/python3.10/site-packages (from statsmodels) (1.23.5)\n",
      "Requirement already satisfied: packaging>=21.3 in ./case-env/lib/python3.10/site-packages (from statsmodels) (24.1)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in ./case-env/lib/python3.10/site-packages (from statsmodels) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in ./case-env/lib/python3.10/site-packages (from statsmodels) (0.5.6)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in ./case-env/lib/python3.10/site-packages (from statsmodels) (1.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./case-env/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./case-env/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./case-env/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: six in ./case-env/lib/python3.10/site-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy==1.23.5 in ./case-env/lib/python3.10/site-packages (1.23.5)\n",
      "Requirement already satisfied: autograd==1.5.0 in ./case-env/lib/python3.10/site-packages (1.5)\n",
      "Requirement already satisfied: lifelines==0.27.0 in ./case-env/lib/python3.10/site-packages (0.27.0)\n",
      "Requirement already satisfied: future>=0.15.2 in ./case-env/lib/python3.10/site-packages (from autograd==1.5.0) (1.0.0)\n",
      "Requirement already satisfied: matplotlib>=3.0 in ./case-env/lib/python3.10/site-packages (from lifelines==0.27.0) (3.9.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in ./case-env/lib/python3.10/site-packages (from lifelines==0.27.0) (1.14.1)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in ./case-env/lib/python3.10/site-packages (from lifelines==0.27.0) (0.5.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in ./case-env/lib/python3.10/site-packages (from lifelines==0.27.0) (2.2.2)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in ./case-env/lib/python3.10/site-packages (from lifelines==0.27.0) (1.0.2)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in ./case-env/lib/python3.10/site-packages (from formulaic>=0.2.2->lifelines==0.27.0) (1.3.0)\n",
      "Requirement already satisfied: wrapt>=1.0 in ./case-env/lib/python3.10/site-packages (from formulaic>=0.2.2->lifelines==0.27.0) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./case-env/lib/python3.10/site-packages (from formulaic>=0.2.2->lifelines==0.27.0) (4.12.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./case-env/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./case-env/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./case-env/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (3.1.4)\n",
      "Requirement already satisfied: pillow>=8 in ./case-env/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (10.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./case-env/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./case-env/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./case-env/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./case-env/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines==0.27.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./case-env/lib/python3.10/site-packages (from pandas>=1.0.0->lifelines==0.27.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./case-env/lib/python3.10/site-packages (from pandas>=1.0.0->lifelines==0.27.0) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./case-env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines==0.27.0) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "1. Data Overview and Cleaning\n",
      "-----------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23772 entries, 0 to 23771\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   experiment_variant      23772 non-null  object        \n",
      " 1   event_name              23772 non-null  object        \n",
      " 2   user_id                 23772 non-null  object        \n",
      " 3   first_event_time        23772 non-null  datetime64[ns]\n",
      " 4   event_time              23772 non-null  datetime64[ns]\n",
      " 5   properties              23772 non-null  object        \n",
      " 6   days_since_first_event  23772 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(1), object(4)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "\n",
      "Missing values:\n",
      "experiment_variant        0\n",
      "event_name                0\n",
      "user_id                   0\n",
      "first_event_time          0\n",
      "event_time                0\n",
      "properties                0\n",
      "days_since_first_event    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Remove any duplicate events\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_duplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Ensure all numeric columns are properly typed\u001b[39;00m\n\u001b[1;32m     48\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevenue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevenue\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/codeway/case-env/lib/python3.10/site-packages/pandas/core/frame.py:6818\u001b[0m, in \u001b[0;36mDataFrame.drop_duplicates\u001b[0;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6815\u001b[0m inplace \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(inplace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6816\u001b[0m ignore_index \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(ignore_index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 6818\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduplicated\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   6819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[1;32m   6820\u001b[0m     result\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(result))\n",
      "File \u001b[0;32m~/Downloads/codeway/case-env/lib/python3.10/site-packages/pandas/core/frame.py:6958\u001b[0m, in \u001b[0;36mDataFrame.duplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   6956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6957\u001b[0m     vals \u001b[38;5;241m=\u001b[39m (col\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m name, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m subset)\n\u001b[0;32m-> 6958\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   6960\u001b[0m     ids \u001b[38;5;241m=\u001b[39m get_group_index(labels, \u001b[38;5;28mtuple\u001b[39m(shape), sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, xnull\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   6961\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(duplicated(ids, keep), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Downloads/codeway/case-env/lib/python3.10/site-packages/pandas/core/frame.py:6926\u001b[0m, in \u001b[0;36mDataFrame.duplicated.<locals>.f\u001b[0;34m(vals)\u001b[0m\n\u001b[1;32m   6925\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(vals) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m-> 6926\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mlen\u001b[39m(shape)\n",
      "File \u001b[0;32m~/Downloads/codeway/case-env/lib/python3.10/site-packages/pandas/core/algorithms.py:795\u001b[0m, in \u001b[0;36mfactorize\u001b[0;34m(values, sort, use_na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[38;5;66;03m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[1;32m    793\u001b[0m             values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[0;32m--> 795\u001b[0m     codes, uniques \u001b[38;5;241m=\u001b[39m \u001b[43mfactorize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    802\u001b[0m     uniques, codes \u001b[38;5;241m=\u001b[39m safe_sort(\n\u001b[1;32m    803\u001b[0m         uniques,\n\u001b[1;32m    804\u001b[0m         codes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m         verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    808\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/codeway/case-env/lib/python3.10/site-packages/pandas/core/algorithms.py:595\u001b[0m, in \u001b[0;36mfactorize_array\u001b[0;34m(values, use_na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[1;32m    592\u001b[0m hash_klass, values \u001b[38;5;241m=\u001b[39m _get_hashtable_algo(values)\n\u001b[1;32m    594\u001b[0m table \u001b[38;5;241m=\u001b[39m hash_klass(size_hint \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values))\n\u001b[0;32m--> 595\u001b[0m uniques, codes \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_sentinel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[1;32m    604\u001b[0m uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7281\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7195\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Comprehensive A/B Test Analysis for Monetization Strategy\n",
    "!pip install statsmodels\n",
    "!pip install numpy==1.23.5 autograd==1.5.0 lifelines==0.27.0\n",
    "\n",
    "# Revised A/B Test Analysis for Monetization Strategy\n",
    "\n",
    "# Revised A/B Test Analysis for Monetization Strategy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Set up basic plot aesthetics\n",
    "plt.style.use('default')  # Use the default Matplotlib style\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# Set a color palette manually\n",
    "color_palette = ['#1f77b4', '#ff7f0e']  # Blue and Orange\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'], unit='ms')\n",
    "    df['first_event_time'] = pd.to_datetime(df['first_event_time'], unit='ms')\n",
    "    df['days_since_first_event'] = (df['event_time'] - df['first_event_time']).dt.total_seconds() / (24 * 60 * 60)\n",
    "    return df\n",
    "\n",
    "df = load_data('data/dataset_experiment.json')\n",
    "\n",
    "# Continue with the rest of the analysis...\n",
    "# 1. Data Overview and Cleaning\n",
    "print(\"1. Data Overview and Cleaning\")\n",
    "print(\"-----------------------------\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove any duplicate events\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Ensure all numeric columns are properly typed\n",
    "df['revenue'] = pd.to_numeric(df['revenue'], errors='coerce')\n",
    "\n",
    "print(\"\\nUnique event types:\")\n",
    "print(df['event_name'].value_counts())\n",
    "\n",
    "# 2. Experiment Setup Analysis\n",
    "print(\"\\n2. Experiment Setup Analysis\")\n",
    "print(\"-----------------------------\")\n",
    "experiment_duration = (df['first_event_time'].max() - df['first_event_time'].min()).days\n",
    "print(f\"Experiment duration: {experiment_duration} days\")\n",
    "\n",
    "variant_counts = df['experiment_variant'].value_counts()\n",
    "print(\"\\nDistribution of experiment variants:\")\n",
    "print(variant_counts)\n",
    "\n",
    "# Chi-square test for equal distribution\n",
    "chi2, p_value = stats.chisquare(variant_counts)\n",
    "print(f\"\\nChi-square test for equal distribution: p-value = {p_value:.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "variant_counts.plot(kind='bar', color=color_palette)\n",
    "plt.title('Distribution of Experiment Variants')\n",
    "plt.xlabel('Variant')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 3. User Acquisition and Retention\n",
    "print(\"\\n3. User Acquisition and Retention\")\n",
    "print(\"----------------------------------\")\n",
    "df_users = df.groupby('user_id').agg({\n",
    "    'first_event_time': 'min',\n",
    "    'event_time': 'max',\n",
    "    'experiment_variant': 'first'\n",
    "}).reset_index()\n",
    "df_users['retention_days'] = (df_users['event_time'] - df_users['first_event_time']).dt.total_seconds() / (24 * 60 * 60)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df_users, x='first_event_time', hue='experiment_variant', multiple='stack', bins=30)\n",
    "plt.title('User Acquisition Over Time')\n",
    "plt.xlabel('First Event Time')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.show()\n",
    "\n",
    "# 4. Conversion Analysis\n",
    "print(\"\\n4. Conversion Analysis\")\n",
    "print(\"----------------------\")\n",
    "def get_conversion_rate(df, variant):\n",
    "    total_users = df[df['experiment_variant'] == variant]['user_id'].nunique()\n",
    "    converted_users = df[(df['experiment_variant'] == variant) & (df['event_name'] == 'subscribe')]['user_id'].nunique()\n",
    "    return converted_users / total_users\n",
    "\n",
    "conv_rate_a = get_conversion_rate(df, 'A')\n",
    "conv_rate_b = get_conversion_rate(df, 'B')\n",
    "\n",
    "print(f\"Conversion rate for Variant A: {conv_rate_a:.2%}\")\n",
    "print(f\"Conversion rate for Variant B: {conv_rate_b:.2%}\")\n",
    "\n",
    "# Z-test for conversion rates\n",
    "total_users_a = df[df['experiment_variant'] == 'A']['user_id'].nunique()\n",
    "total_users_b = df[df['experiment_variant'] == 'B']['user_id'].nunique()\n",
    "converted_users_a = df[(df['experiment_variant'] == 'A') & (df['event_name'] == 'subscribe')]['user_id'].nunique()\n",
    "converted_users_b = df[(df['experiment_variant'] == 'B') & (df['event_name'] == 'subscribe')]['user_id'].nunique()\n",
    "\n",
    "z_stat, p_value = stats.proportions_ztest([converted_users_a, converted_users_b], [total_users_a, total_users_b])\n",
    "print(f\"Z-test for conversion rates: p-value = {p_value:.4f}\")\n",
    "\n",
    "# 5. Revenue and LTV Analysis\n",
    "print(\"\\n5. Revenue and LTV Analysis\")\n",
    "print(\"---------------------------\")\n",
    "def calculate_ltv(group):\n",
    "    revenue = group[group['event_name'] == 'subscribe']['revenue'].sum() - \\\n",
    "              group[group['event_name'] == 'refund']['revenue'].sum()\n",
    "    return pd.Series({'ltv': revenue})\n",
    "\n",
    "user_ltv = df.groupby(['user_id', 'experiment_variant']).apply(calculate_ltv).reset_index()\n",
    "\n",
    "ltv_by_variant = user_ltv.groupby('experiment_variant')['ltv'].agg(['mean', 'median', 'count'])\n",
    "print(\"LTV by Variant:\")\n",
    "print(ltv_by_variant)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='experiment_variant', y='ltv', data=user_ltv)\n",
    "plt.title('Distribution of LTV by Variant')\n",
    "plt.xlabel('Variant')\n",
    "plt.ylabel('LTV (USD)')\n",
    "plt.show()\n",
    "\n",
    "# Mann-Whitney U test for LTV\n",
    "variant_a = user_ltv[user_ltv['experiment_variant'] == 'A']['ltv']\n",
    "variant_b = user_ltv[user_ltv['experiment_variant'] == 'B']['ltv']\n",
    "statistic, p_value = stats.mannwhitneyu(variant_a, variant_b, alternative='two-sided')\n",
    "\n",
    "print(f\"Mann-Whitney U test for LTV: p-value = {p_value:.4f}\")\n",
    "\n",
    "# 6. Subscription Duration Analysis\n",
    "print(\"\\n6. Subscription Duration Analysis\")\n",
    "print(\"----------------------------------\")\n",
    "subscription_durations = df[df['event_name'] == 'subscribe']['productDuration'].value_counts()\n",
    "print(\"Distribution of subscription durations:\")\n",
    "print(subscription_durations)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='productDuration', hue='experiment_variant', data=df[df['event_name'] == 'subscribe'])\n",
    "plt.title('Distribution of Subscription Durations by Variant')\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Chi-square test for subscription duration distribution\n",
    "duration_counts = pd.crosstab(df[df['event_name'] == 'subscribe']['productDuration'], \n",
    "                              df[df['event_name'] == 'subscribe']['experiment_variant'])\n",
    "chi2, p_value, dof, expected = stats.chi2_contingency(duration_counts)\n",
    "print(f\"Chi-square test for subscription duration distribution: p-value = {p_value:.4f}\")\n",
    "\n",
    "# 7. Churn and Retention Analysis\n",
    "print(\"\\n7. Churn and Retention Analysis\")\n",
    "print(\"--------------------------------\")\n",
    "def get_churn_rate(df, variant):\n",
    "    subscribed_users = df[(df['experiment_variant'] == variant) & (df['event_name'] == 'subscribe')]['user_id'].nunique()\n",
    "    churned_users = df[(df['experiment_variant'] == variant) & (df['event_name'] == 'auto_renew_off')]['user_id'].nunique()\n",
    "    return churned_users / subscribed_users\n",
    "\n",
    "churn_rate_a = get_churn_rate(df, 'A')\n",
    "churn_rate_b = get_churn_rate(df, 'B')\n",
    "\n",
    "print(f\"Churn rate for Variant A: {churn_rate_a:.2%}\")\n",
    "print(f\"Churn rate for Variant B: {churn_rate_b:.2%}\")\n",
    "\n",
    "# Z-test for churn rates\n",
    "subscribed_users_a = df[(df['experiment_variant'] == 'A') & (df['event_name'] == 'subscribe')]['user_id'].nunique()\n",
    "subscribed_users_b = df[(df['experiment_variant'] == 'B') & (df['event_name'] == 'subscribe')]['user_id'].nunique()\n",
    "churned_users_a = df[(df['experiment_variant'] == 'A') & (df['event_name'] == 'auto_renew_off')]['user_id'].nunique()\n",
    "churned_users_b = df[(df['experiment_variant'] == 'B') & (df['event_name'] == 'auto_renew_off')]['user_id'].nunique()\n",
    "\n",
    "z_stat, p_value = stats.proportions_ztest([churned_users_a, churned_users_b], [subscribed_users_a, subscribed_users_b])\n",
    "print(f\"Z-test for churn rates: p-value = {p_value:.4f}\")\n",
    "\n",
    "# 8. Refund Analysis\n",
    "print(\"\\n8. Refund Analysis\")\n",
    "print(\"------------------\")\n",
    "def get_refund_rate(df, variant):\n",
    "    subscribed_users = df[(df['experiment_variant'] == variant) & (df['event_name'] == 'subscribe')]['user_id'].nunique()\n",
    "    refunded_users = df[(df['experiment_variant'] == variant) & (df['event_name'] == 'refund')]['user_id'].nunique()\n",
    "    return refunded_users / subscribed_users\n",
    "\n",
    "refund_rate_a = get_refund_rate(df, 'A')\n",
    "refund_rate_b = get_refund_rate(df, 'B')\n",
    "\n",
    "print(f\"Refund rate for Variant A: {refund_rate_a:.2%}\")\n",
    "print(f\"Refund rate for Variant B: {refund_rate_b:.2%}\")\n",
    "\n",
    "# Z-test for refund rates\n",
    "refunded_users_a = df[(df['experiment_variant'] == 'A') & (df['event_name'] == 'refund')]['user_id'].nunique()\n",
    "refunded_users_b = df[(df['experiment_variant'] == 'B') & (df['event_name'] == 'refund')]['user_id'].nunique()\n",
    "\n",
    "z_stat, p_value = stats.proportions_ztest([refunded_users_a, refunded_users_b], [subscribed_users_a, subscribed_users_b])\n",
    "print(f\"Z-test for refund rates: p-value = {p_value:.4f}\")\n",
    "\n",
    "# 9. Time-based Analysis\n",
    "print(\"\\n9. Time-based Analysis\")\n",
    "print(\"----------------------\")\n",
    "df_time = df.set_index('event_time').resample('D').agg({\n",
    "    'revenue': 'sum',\n",
    "    'user_id': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "df_time['cumulative_revenue'] = df_time.groupby('experiment_variant')['revenue'].cumsum()\n",
    "df_time['cumulative_users'] = df_time.groupby('experiment_variant')['user_id'].cumsum()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='event_time', y='cumulative_revenue', hue='experiment_variant', data=df_time)\n",
    "plt.title('Cumulative Revenue Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Revenue (USD)')\n",
    "plt.show()\n",
    "\n",
    "# 10. Cohort Analysis\n",
    "print(\"\\n10. Cohort Analysis\")\n",
    "print(\"-------------------\")\n",
    "df['cohort'] = df['first_event_time'].dt.to_period('W')\n",
    "df['week'] = (df['event_time'].dt.to_period('W') - df['cohort']).apply(lambda r: r.n)\n",
    "\n",
    "cohort_data = df.groupby(['cohort', 'week', 'experiment_variant'])['user_id'].nunique().unstack(level='experiment_variant')\n",
    "retention_data = cohort_data.div(cohort_data.iloc[:, 0], axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(retention_data, annot=True, fmt='.2%', cmap='YlGnBu')\n",
    "plt.title('Cohort Retention Heatmap')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Cohort')\n",
    "plt.show()\n",
    "\n",
    "# 11. LTV Projection\n",
    "print(\"\\n11. LTV Projection\")\n",
    "print(\"------------------\")\n",
    "def project_ltv(df, days):\n",
    "    df_proj = df[df['days_since_first_event'] <= days].copy()\n",
    "    user_ltv = df_proj.groupby(['user_id', 'experiment_variant']).apply(calculate_ltv).reset_index()\n",
    "    return user_ltv.groupby('experiment_variant')['ltv'].mean()\n",
    "\n",
    "ltv_30 = project_ltv(df, 30)\n",
    "ltv_90 = project_ltv(df, 90)\n",
    "ltv_180 = project_ltv(df, 180)\n",
    "ltv_365 = project_ltv(df, 365)\n",
    "\n",
    "print(\"Projected LTV by variant:\")\n",
    "print(pd.DataFrame({\n",
    "    '30 days': ltv_30,\n",
    "    '90 days': ltv_90,\n",
    "    '180 days': ltv_180,\n",
    "    '365 days': ltv_365\n",
    "}))\n",
    "\n",
    "# 12. Conclusion and Recommendations\n",
    "print(\"\\n12. Conclusion and Recommendations\")\n",
    "print(\"-----------------------------------\")\n",
    "winning_variant = 'B' if ltv_by_variant.loc['B', 'mean'] > ltv_by_variant.loc['A', 'mean'] else 'A'\n",
    "\n",
    "print(f\"Based on our comprehensive analysis, Variant {winning_variant} appears to be the winning variant in terms of overall Lifetime Value (LTV).\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"1. LTV: Variant {winning_variant} shows a higher average LTV.\")\n",
    "print(f\"2. Conversion Rate: Variant {'B' if conv_rate_b > conv_rate_a else 'A'} has a higher conversion rate.\")\n",
    "print(f\"3. Churn Rate: Variant {'B' if churn_rate_b < churn_rate_a else 'A'} demonstrates a lower churn rate.\")\n",
    "print(f\"4. Refund Rate: Variant {'B' if refund_rate_b < refund_rate_a else 'A'} has a lower refund rate.\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\nThe difference in LTV between variants is statistically significant (p-value: {p_value:.4f}).\")\n",
    "else:\n",
    "    print(f\"\\nThe difference in LTV between variants is not statistically significant (p-value: {p_value:.4f}).\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "if p_value < 0.05:\n",
    "    print(f\"1. Implement Variant {winning_variant}: Given the statistically significant results, we recommend rolling out Variant {winning_variant} to all users.\")\n",
    "    print(f\"2. Further Optimization: Conduct follow-up experiments to fine-tune Variant {winning_variant} and potentially improve its performance even more.\")\n",
    "else:\n",
    "    print(\"1. Extended Testing: While there are observable differences, they are not statistically significant. Consider extending the experiment duration or increasing the sample size to achieve statistical significance.\")\n",
    "    print(\"2. Segment Analysis: Conduct a deeper analysis of user segments to identify any subgroups where one variant clearly outperforms the other.\")\n",
    "\n",
    "print(\"\\n3. User Experience Analysis: Investigate the factors contributing to the differences in conversion, churn, and refund rates between variants. This may involve user surveys or session recording analysis.\")\n",
    "print(\"4. LTV Projection Refinement: Continuously refine the LTV projection model as more long-term data becomes available. This will improve future decision-making and experiment evaluation.\")\n",
    "print(\"5. Cohort Analysis Follow-up: Pay close attention to the performance of recent cohorts. If newer cohorts show stronger preference for one variant, it may indicate shifting user preferences or market conditions.\")\n",
    "print(\"6. Subscription Duration Strategy: Based on the distribution of subscription durations, consider adjusting the offering or pricing strategy to encourage longer subscription periods in the winning variant.\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Present findings to key stakeholders and decide on the implementation strategy.\")\n",
    "print(\"2. Develop a roadmap for future experiments based on the insights gained from this test.\")\n",
    "print(\"3. Set up a system for continuous monitoring of key metrics (LTV, conversion rate, churn rate) post-implementation.\")\n",
    "print(\"4. Conduct a retrospective on the A/B testing process to identify areas for improvement in future experiments.\")\n",
    "\n",
    "# Visualization of Key Metrics\n",
    "metrics = pd.DataFrame({\n",
    "    'Metric': ['LTV', 'Conversion Rate', 'Churn Rate', 'Refund Rate'],\n",
    "    'Variant A': [ltv_by_variant.loc['A', 'mean'], conv_rate_a, churn_rate_a, refund_rate_a],\n",
    "    'Variant B': [ltv_by_variant.loc['B', 'mean'], conv_rate_b, churn_rate_b, refund_rate_b]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics.set_index('Metric').plot(kind='bar')\n",
    "plt.title('Comparison of Key Metrics Between Variants')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(title='Variant')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final LTV Projection Visualization\n",
    "projection_data = pd.DataFrame({\n",
    "    'Days': [30, 90, 180, 365],\n",
    "    'Variant A': [ltv_30['A'], ltv_90['A'], ltv_180['A'], ltv_365['A']],\n",
    "    'Variant B': [ltv_30['B'], ltv_90['B'], ltv_180['B'], ltv_365['B']]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Days', y='value', hue='variable', data=pd.melt(projection_data, ['Days']))\n",
    "plt.title('LTV Projection Over Time')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Projected LTV (USD)')\n",
    "plt.legend(title='Variant')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThis concludes our comprehensive A/B test analysis. The visualizations above summarize our key findings and projections.\")\n",
    "\n",
    "# Additional analysis: Effect size calculation\n",
    "ltv_mean_diff = ltv_by_variant.loc['B', 'mean'] - ltv_by_variant.loc['A', 'mean']\n",
    "ltv_pooled_std = np.sqrt((ltv_by_variant.loc['A', 'count'] * variant_a.std()**2 + \n",
    "                          ltv_by_variant.loc['B', 'count'] * variant_b.std()**2) / \n",
    "                         (ltv_by_variant.loc['A', 'count'] + ltv_by_variant.loc['B', 'count'] - 2))\n",
    "effect_size = ltv_mean_diff / ltv_pooled_std\n",
    "\n",
    "print(f\"\\nEffect Size (Cohen's d) for LTV difference: {effect_size:.4f}\")\n",
    "\n",
    "if abs(effect_size) < 0.2:\n",
    "    print(\"This indicates a small effect size.\")\n",
    "elif abs(effect_size) < 0.5:\n",
    "    print(\"This indicates a medium effect size.\")\n",
    "else:\n",
    "    print(\"This indicates a large effect size.\")\n",
    "\n",
    "print(\"\\nFinal Thoughts:\")\n",
    "print(\"1. Statistical vs. Practical Significance: While we've focused on statistical significance, it's crucial to consider the practical significance of the observed differences. Even if not statistically significant, the differences in key metrics may still be practically important for the business.\")\n",
    "print(\"2. Long-term Impact: The LTV projections suggest that the differences between variants may compound over time. Continue monitoring long-term trends to validate these projections.\")\n",
    "print(\"3. Balancing Metrics: While LTV is our primary metric, it's important to consider the balance between all metrics (conversion rate, churn rate, refund rate) when making the final decision.\")\n",
    "print(\"4. Iterative Improvement: Regardless of which variant is chosen, there's always room for improvement. Use the insights from this test to inform future iterations and experiments.\")\n",
    "print(\"5. Customer Satisfaction: Don't forget to consider qualitative feedback and customer satisfaction metrics alongside these quantitative results.\")\n",
    "\n",
    "print(\"\\nBy considering all these factors, you'll be well-equipped to make an informed decision about your monetization strategy and set the stage for continued optimization and growth.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415c555-3b95-4be6-8bfe-50f7f19d1102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
